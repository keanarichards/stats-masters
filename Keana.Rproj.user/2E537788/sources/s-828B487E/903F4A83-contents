---
output: pdf_document
---

```{r}
# load packages -----------------------------------------------------------

## Package names
packages <- c("tidyverse", "here", "papaja")

## Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

## Packages loading
invisible(lapply(packages, library, character.only = TRUE))

# load data ---------------------------------------------------------------

clean <- read_csv(here("data", "wide.csv"))


```


## Participants 
527 participants originally completed the survey, and we excluded any participants that alluded to the hypothesis in the suspicion check by responding that the study was about race and/or race and voice perceptions (_N_ = 20; 3.8% of total sample). The final sample consisted of 507 (278 Women, 229 Men) participants from Amazon Mechanical Turk (see Table A1 for demographic information). Ages ranged between 19 and 82 years (_M__age_= 40.07, _SD_ = 13.26). We included participants based upon the following criteria: (a) adults on (b) Amazon Mechanical Turk) (c) born and currently residing in the US (d) have had 90% or greater of their previous HITs approved, and (e) have a device with audio capabilities. We excluded Black individuals during the pre-screening process, since we are primarily interested in understanding the factors that affect threat and leadership perceptions of Black men, and group membership may differentially affect these perceptions.

## Design
The study was a 2X2 within-subjects design with two independent variables: voice pitch (high or low) and race (White or Black names). Each of the four conditions was counterbalanced. Names and individual voices were randomly assigned to participants without repeat. This ensured that individuals would not listen to a high and low voice that resulted from the same original voice.

## Procedure

Participants were recruited from Amazon Mechanical Turk by posting a HIT (human intelligence task) on the site. They were told that they would listen to a participant that previously provided their recording and took a "series of character trait and performance tests," which would then be compared to the participants' ratings to assess the accuracy of their perceptions. Upon being assigned to a recording, they learned the participant's name, and were provided other information about the recording (i.e., location, date) to make the design less conspicuous. Then, they listened to the participants' recording by clicking on the Soundcloud file embedded in the survey.

All of the names were randomly assigned to correspond to the high-pitched or the low-pitched conditions. The presentation of the four names for the recordings was randomized and counterbalanced across participants. We verified that the randomization worked by comparing the number of participants that were assigned to each condition, which were relatively uniform. The four conditions (Black name high pitch, Black name low pitch, White name high pitch, White name low pitch), were equally presented first, second, third, and fourth (see Table A2).

They were asked to assess the participant's character based upon their voice using a series of 100-point slider scale questions (i.e., trustworthiness, dominance, threateningness), which served as our measures of perceived trustworthiness, perceived dominance, and perceived threat, respectively. The presentation of the scale items was counterbalanced for each participant and within each condition. Additionally, we asked them to rate the individuals in the recording on various traits that were independently rated as important for leaders on 100-point slider scale items. Finally, they indicated their preferences for engaging in different types of interdependent relationships with the people in the recording on 100-point slider scales. Participants could listen to the recordings as frequently as they desired before rating the voices. They completed demographic questions and indicated what they thought the study was about as a suspicion check. After participants completed the suspicion check, we determined whether the manipulation of the names elicited perceptions of the race of the recorded individuals through a series of manipulation check questions. First, we created a name attention check score based upon whether the participants remembered the names of the people in the recordings. The participants were presented with a list of eight names, four of which were included in the study. Every time they correctly identified a name that was presented to them during the study, they received a point, for a total name recall score of four points (_M_ = 3.09, _SD_ = .979). If they incorrectly selected a name that was not presented to them, they did not receive a point. On the name attention check, participants recalled the Black names with greater accuracy (68.34%) than White names (59.69%). Also, they remembered the conditions presented first (69.09%), second (70.80%), and fourth (76.30%) better than they remembered the condition presented third (39.88%). Participants were asked how many people in the recordings they thought were White or Black (see Figures A1 and A2). Finally, we asked participants to rate the likelihood that people with the names used in the study would be White or Black (see Table A3). A debriefing page explaining the true purposes of the study and the logic behind the deception was provided before payment. Participants were paid $1.00 for their participation.

## Materials and measures 

### Voice stimuli
For the voice stimuli, we recorded the voices of eight White men between 18-30 years of age in Audacity using the Zoom H4N Handy Recorder with a sampling rate of 44.1kHz. The men quoted the first sentence of the Rainbow Passage (e.g., "When the sunlight strikes raindrops in the air, they act as a prism and form a rainbow") [@Fairbanks1960]. At the end of each sentence, the men read a randomly assigned identification number provided by the researchers. The four-digit identification numbers were created randomly, and participants were required to enter the identification number as a means of verifying that they were listening to the recordings.

After the recording sessions, each voice was manipulated to have a higher or lower pitch in Praat (Version 6.0.36) [@Boersma2001], which served as our manipulation of threat potential through the voice. We followed the standard methods in voice research by raising and lowering each voice by 0.5 equivalent rectangular bandwidths (ERBs) using the Pitch-Synchronous Overlap Add tool in Praat, which produces a shift in perceived pitch of approximately 20 Hz in either direction [e.g., @Apicella2009; @Klofstad2012; @Tigue2012; @Vukovic2010]. We set the pitch floor to 70 Hz and the pitch ceiling to 250 Hz, which has been validated as an appropriate range for male voices [@Vogel2009]. Many researchers manipulate ERB instead of Hertz because a change in pitch is perceived differently depending upon the original pitch that was manipulated, since there is a logarithmic relationship between actual pitch and perceived pitch (Stevens, 1998). Also, the ERB manipulations will not affect other acoustic characteristics of the recording (e.g., rate, intensity) [@Feinberg2005]. Since each of the voices was raised and lowered in pitch, there were a total of sixteen manipulated recordings. We checked the manipulation by comparing the mean pitch for the original voices (_M_ = 104.37, _SD_ = 14.09) to the lower manipulations (_M_ = 90.21, _SD_ = 9.79) and the higher manipulations (_M_ = 121.30, _SD_ = 17.28). All of the manipulated files were uploaded to separate Soundcloud links and embedded in the survey.

### Names for race manipulation
To manipulate perceptions of race, we used four names that are typically associated with Black people (i.e., Tyrone, Keyshawn, Deshawn, Terrell) and four names that are typically associated with White people (i.e., Scott, Brad, Brett, and Logan) [@Gaddis2017]. Names were presented before the participants listened to the voice recording. Each name was chosen based upon the criteria that 90% or more of raters from @Gaddis2017 thought that the individual was either Black or White when they were asked about their perceptions of the person's race based upon their name.

### Perceived leadership ability
We recruited 55 participants on Amazon Mechanical Turk to serve as independent raters for identifying the leadership traits used in the experiment. We provided them with a list of fifteen traits from which they could select what they considered most valuable for successful leaders of businesses and companies (e.g., drive, creativity, confidence) [@Kirkpatrick1991]. We selected the traits that were ranked, on average, in the 30th percentile of responses (where 1 is considered the most important trait for a leader). The traits that were selected for the leadership composite score based upon these criteria were intelligence, effective communication, confidence, and problem-solving ability, which were rated by participants in the final study using 100-point slider scale items.

To create the leadership ability composite, we averaged participants' ratings of the individual in the recording on the four traits. Higher scores denote greater perceived leadership ability. The measure had high internal consistency across participants in the final sample (alpha = .91; averaged across all conditions).

### Perceived threat, trustworthiness, and dominance
Single questions were used to elicit perceived threat, trustworthiness, and dominance. Participants responded using a 100-point slider scale.

## Multilevel models 

Given the nature of the data (condition nested within subjects and subjects nested within the "names" associated with each voice), we employed multilevel models (also known as linear mixed-effects models or hierarchical linear models) [@Finch2014; @Raudenbush2002; @Gelman2007; @Garson2013; @Galecki2013] to analyze the data, when feasible. The basic premise of using this type of analysis is to account for the inherent correlation among the observations nested within other variables. For instance, within the context of the current study, we measured participants' rating of threat across all conditions, so it is likely perceptions of threat within each participant will be correlated, since there may be inherent individual differences in participants' baseline perceptions of others' threat and/or perceptions of threat in response to each combination of race and voice pitch. If we did not account for this correlation in responses within each participant, we would be violating the assumption of independence of observations. Repeated measures analysis of variance (ANOVA) is often used for analyzing data of this nature, however we use multilevel models because they present several notable advantages over repeated measures ANOVA. For instance, multilevel models are more powerful in the face of "unbalanced" repeats, where the measure of interest is missing one or more observations (within reason). Instead of employing listwise deletion in the face of missing data points during analysis, like repeated measure ANOVA, which reduces the effective sample size, multilevel models use the data available within a group to estimate parameters and compute inferential statistics, while accounting for the fact that some estimates are more reliable than others [@Raudenbush2002; @Brauer2018; @Misangyi2006]. Notably, we assume that the data points are missing at random for these inferences. Additionally, multilevel models allow researchers to explore multiple groups with correlated observations (that is, multiple sources of nonindependence) [@Brauer2018; @Westfall2014; @Westfall2015], while repeated measures ANOVAs only allow one to account for one source of nonindependence [e.g., @Baayen2008; @Judd2017]. The effects of continuous predictors that may be clustered can only be analyzed using multilevel models, since repeated measures ANOVAs only accept within-group (e.g., subject) factors as predictors [@Misangyi2006; @Brauer2018]. Finally, multilevel models allow researchers to explicitly model different sources of variation within the data (e.g., individual and group-level variation in group-level estimates, variation in individual-level estimates) and estimate the effects for specific groups [@Gelman2007]. 

### Model estimation and comparison

Multilevel models are a variation of classical regression that assign a probability model to specific regression coefficients [@Gelman2007]. The parameters of the second-level model have their own coefficients, known as hyperparameters [@Gelman2007]. Although classical regression has the capacity to model varying coefficients with the use of indicator variables, multilevel models are unique in their ability to model the variation between groups by including varying coefficients and models for each varying coefficient [@Gelman2007]. To model variation at multiple levels, these models incorporate what are known as "fixed" and "random" effects, where random effects are typically conceptualized as effects that vary across the nested groups, while fixed effects are constant across all groups within the data [@Finch2014]. 

When analyzing data using multilevel modeling, there are different random effects structures that can be used to model the data. A random effects structure is essentially the way the parameters are assumed to vary across the nested groups [@Barr2013]. The most basic random effects structure includes only a random intercept, which essentially allows the intercept to vary across groups (i.e., there is a different intercept estimated for each group). A more complex random effects structure involves allowing the slopes (i.e., fitting a unique regression line to each group) and intercepts to vary by group. There are many different ways to model the random effects structure (e.g., random slope by group with correlated intercepts, random slope by group without varying intercepts by group, uncorrelated random intercept and slopes by group, etc.) [@Meteyard2020], which will change the interpretation of the results and even reduce power or increase Type 1 error [@Barr2013; @Matuschek2017], so it is important to identify a random effects structure that is appropriate for the data. 

The literature on determining random effects structure for one's data is mixed. Some have argued that it is imperative to fit maximal models (i.e., fit random slopes and intercepts for each predictor in the model, including interactions) whenever possible, and only reduce the random effects structure when the model does not converge [@Barr2013]. When a model does not converge, it essentially means that the optimization algorithm used to estimate parameters cannot reliably determine the maximum likelihood function for the current model [@Brauer2018]. This typically occurs when there is insufficient data for the number of parameters being estimated. Thus, failures to converge are much more likely to occur when trying to estimate maximal models. On the other hand, fitting models with only intercepts varying by group leads to inflated Type 1 error [@Schielzeth2009; @Barr2013]. Thus, @Barr2013 argue that the common practice of fitting varying-intercept only models can lead to biased conclusions and instead argued for maximal models. However, it has been argued that the rise in employing "maximal" models can lead to their own set of problems, namely i) failure to converge [@Bates2015], ii) models that converge but are so overparameterized that they are uninterpretable [@Bates2015], and iii) loss of power due to random effects contributing little to the model [@Matuschek2017]. In place of "maximal" models, @Matuschek2017 and @Bates2015 argue for "parsimonious mixed models," where the researcher uses a pre-determined model comparison technique (e.g., likelihood ratio test, Akaike information criterion, Bayes/Schwarz information criterion) to select the random effects structure that best fits the data. For selecting parsimonious mixed models, one would first fit a maximal model, then remove random effects that are not contributing to the model (i.e., variance is close to 0), stopping before they reach a model that would significantly reduce the goodness of fit [@Matuschek2017; @Bates2015]. In support of this argument, @Matuschek2017 use simulations to demonstrate that parsimonious models can reduce Type 1 error associated with underfitting models, while attaining higher power than maximal models. This is because maximal models can lead to a decrease in statistical power if they have random effects with variances near 0 which do not contribute to the fit of the model but reduce the degrees of freedom, essentially increasing the standard errors of the fixed effects estimates [@Matuschek2017]. At the same time, it is generally accepted that random effects with near-zero variance do not affect goodness of fit tests [@Brauer2018].  


Despite the lack of consensus regarding how to determine the final model, most researchers suggest starting with the maximal model and that the final model should have an effects structure that aligns with the researcher's main hypothesis, even if these random effects have near-zero variance [@Brauer2018; @Barr2013; @Bates2015]. 

Another general point of consensus is that when fitting models with varying effects, one should use restricted estimated likelihood (REML) instead of maximum likelihood (ML) estimation for unbiased estimates of the random effects parameters [@Brauer2018; @Maas2005; @Browne2006; @Elff2020;  @Gelman2007], especially with smaller samples at the group-level [@Hox2020;@Mcneish2016a;@McNeish2017]. The problem with ML estimation typically arises with smaller samples because the process of ML estimation tends to ignore variability in the fixed effect estimates and does not account for the degrees of freedom (DF) used to estimate the fixed effects [@McNeish2017]. These effects of using ML can lead to more bias in random effects estimation with smaller samples because they are more sensitive to small changes in the degrees of freedom and tend to have larger sampling variability. Since random effects parameters are estimated based on the fixed effects parameters, this can cause them to be underestimated [@McNeish2017]. As a result, the standard errors of the fixed effects tend to be underestimated because the random effects estimates are integrated into the formula for fixed effects standard errors. With smaller standard errors, the t or Z test statistic will be overestimated, leading to higher Type 1 error. The process employed by REML leads to better estimates of the random effects, which in turn improves the fixed effects standard error estimates [@McNeish2017]. 

Finally, researchers have examined how different techniques for evaluating significance of multilevel models affect Type 1 error rates. Notably, @Luke2017 show through simulations that likelihood ratio tests (LRTs) and applying the *Z* distribution to the Wald *t* values from the model output can lead to unusually high Type 1 error rates, especially with smaller samples (i.e., less than 40-50 number of items and/or subjects). This is the case when fitting models using both ML and REML. Of the options available to researchers in R, Type 1 error is closest to .05 when deriving p-values using Kenward-Roger [@Kenward1997] or Satterthwaite [@Satterthwaite1941] corrections for approximating for denominator degrees of freedom for F statistics or DF for t statistics [@Luke2017]. Although these corrections tend to produce similar output [@Luke2017], @Hox2020 and @McNeish2017 argue that the Kenward-Roger provides slightly better approximations by correcting standard error and estimating DF, while the Satterthwaite correction only estimates the effective DF. 

### Sample size considerations

Another important consideration in determining model structure is the sample size at each level (i.e., number of groups and number of individuals within each group) of multilevel models. Like in most parametric statistical inference, the estimates become unreliable (and in some cases, impossible to estimate) with sparse data. These effects may differ depending on the level, where @Scherbaum2009 showed that increasing level-2 (i.e., number of groups) sample size had a larger effect on variance components than increasing level-1 (i.e., number of individuals within each group) sample size. 

Understanding how "sparse" one's data can be at each level while being able to maintain unbiased estimates has been the subject of several lines of recent work. One of the seminal pieces in this literature suggested that level-2 standard errors are biased when the sample size is less than 50 (i.e., there are fewer than 50 groups) [@Maas2005]. Other recommended standards are to have 10 observations for at least 100 groups to estimate a random intercept for said group, and at least 20 observations with a minimum of 200 groups for estimating slope variance [@Clarke2007]. @Scherbaum2009 recommends 30-50 trials/items per participant for power. Notably, the size of each cluster can affect estimation of the random effects, but tend to have little to no impact on estimation of fixed effects [@Clarke2007; @Maas2004; @Maas2005]. 

Thus, recent work has focused primarily on the effects of small samples on the estimation of random effects. As of recently, many researchers are suggesting that, under certain conditions (e.g., continuous outcome variables, five or fewer fixed effects, no missing data, one or two variance components), there can be as few as 7-10 groups at the second level to be able to estimate random effects with reasonable accuracy using REML with cross-sectional data. However, the appropriate sample size will intrinsically depend on the nature of the data and model at hand [@Hox2020]. 

With small samples, some suggest that Bayesian estimation can be a more accurate alternative [@Stegmueller2013] because Bayesian statistics do not rely on the central limit theorem [@Hox2020]. Specifically, @Stegmueller2013 performed a Monte Carlo experiment to compare the performance of frequentist and Bayesian multilevel models when there are few (e.g., 5, 10, 15) groups, and showed that the frequentist approach tended to be anti-conservative and biased with smaller samples. However, in a response to @Stegmueller2013, @Elff2020 has recently argued against the notion that standard multilevel models are inferior to models following a Bayesian framework with a small number of groups. They showed that the estimation bias found in @Stegmueller2013 could be solved by simply using i) REML estimators for variance parameters and ii) a t-distribution with appropriate degrees of freedom for statistical inference. Through these steps, the standard multilevel models were found to produce unbiased estimates of both fixed and random effects. 

Additionally, any possible advantages of Bayesian estimation are completely dependent on the choice in a prior probability distribution (often simply called a prior) [@Gelman2007; @Hox2020]. A prior essentially represents the knowledge, a priori, one has (if any) about the distribution of the parameters, which are then combined with the data observed to produce posterior inferences. Thus, the choice of a prior probability distribution is critical, especially with smaller samples [@Mcneish2016]. For instance, in a systematic review of the literature on estimation while using small samples, @Smid2020 showed that inference with uninformative priors can lead to estimates that are just as, if not more, biased, than the estimates from frequentist methods when working with a small number of groups. Unfortunately, the default prior distribution in most software is uninformative, so it is entirely possible that many researchers acquire biased estimates by naively using Bayesian estimation [@Hox2020]. 

### Multilevel modeling methodology for current research

To conduct analyses for the current work, we employed many of the techniques and recommendations described above. First, we used `r cite_r()` for all analyses [see here for all data and analysis scripts](https://github.com/keanarichards/stats-masters -- make sure to make this public) (see appendix for full list of R packages used). To determine the maximal random effects structure, we follow the guidance of @Brauer2018 and @Barr2013, who argue that i) every source of nonindependence should be modeled through a random intercept ii) generally, there should be a random slope for each within-unit predictor and, iii) one should estimate random slopes for interaction effects when all factors comprising the interaction are within-group. They also note that there are exceptions to these general rules of thumb. For instance, i) does not need to be followed if the purported random effect is fully confounded with a predictor in the model. That is, we would exclude groups where the random variable is nested within a fixed effect. Since the names chosen for the current experiment were necessarily nested within race, we do not model random effects for name when race is included as a fixed effect. Also, these general guidelines are under the assumption that there are enough data at each level of the model to be able to obtain reasonable estimates. In our case, we have `r nrow(clean)` participants, which means that we are likely well-powered to estimate random effects for each participants (that is, we have `r nrow(clean)` groups for the participant variable), even with four observations within each group. As mentioned before, the second-level sample size is more important in determining the power than level-1 sample size [@Scherbaum2009]. However, we also assume that the voice presented to participants in the recording is a random effect, since it is likely that responses to one voice will be more similar to each other than responses to another voice. One point of concern with obtaining estimates for the voice variable is that the sample is relatively small, with only 8 voices used during the experiment. Since we attempt to estimate random slopes and intercepts for the voice variable and may be underpowered (especially for random slopes), we recommend taking the sample size into consideration when interpreting the estimates for the random effect of voice and encourage future research with larger samples to replicate these results. 

Here, we will describe the steps for selecting our final random effects structure. For each model, we started with a maximal random effects structure, as defined above. Although it is generally recommended to fit a model with a random effects interaction term for any within-subjects fixed effects that have an interaction term [@Barr2013], in this case, we did not have enough data for fitting the interaction between two random within-subjects slopes (i.e., voice pitch and race), because there would have been one data point per cell. Instead, we start with maximal models that have all within-subjects slopes and intercepts possible. Then, we submitted this maximal model to the "buildmer" function from the buildmer package with a bobyqa optimizer to select a final model. Using the maximal model formula provided, the buildmer function enters parameters one-by-one in terms of their contribution to the change in log-likelihood (i.e., terms that have lower chi-square p-values are entered first), stopping when a model does not converge. By following these steps, the function ensures that the most relevant parameters are included in the model before the model fails to converge. After selecting this maximal model, the function follows a backwards step-wise selection process, where terms are eliminated when they do not significantly contribute to the model, that is, when removing the parameter does not cause a significant reduction in the likelihood-ratio (if *p* >= .05) (See Appendix Table X for comparison of maximal and reduced models). Once the final models were selected, we tested the assumptions of each model (i.e., linearity, normal distribution of residuals, homogeneity of variance), which are described in the Appendix for all final models and should be also be considered when interpreting the effects.